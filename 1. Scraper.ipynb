{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "faced-brunei",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import date\n",
    "import re\n",
    "import threading\n",
    "import pickle\n",
    "import sys\n",
    "import os\n",
    "from urllib.parse import urljoin\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import concurrent.futures\n",
    "from __future__ import print_function\n",
    "from urllib.request import urlopen, Request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "civilian-figure",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FedMinScraper(object):\n",
    "    \"\"\"\n",
    "    The purpose of this class is to extract monthly US federal reserve minutes\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    dates: list('yyyy'|'yyyy-mm')\n",
    "        List of strings/integers referencing dates for extraction\n",
    "        Example:\n",
    "        dates = [min_year] -> Extracts all transcripts for this year\n",
    "        dates = [min_year,max_year] -> Extracts transactions for a set of years\n",
    "        dates = ['2020-01'] -> Extracts transcripts for a single month/year\n",
    "    nthreads: int\n",
    "        Set of threads used for multiprocessing\n",
    "        defaults to None\n",
    "    Returns\n",
    "    --------\n",
    "    transcripts: txt files\n",
    "    \"\"\"\n",
    "\n",
    "    url_parent = r\"https://www.federalreserve.gov/monetarypolicy/\"\n",
    "    url_current = r\"fomccalendars.htm\"\n",
    "\n",
    "    # historical transcripts are stored differently\n",
    "    url_historical = r\"fomchistorical{}.htm\"\n",
    "    # each transcript has a unique address, gathered from url_current or url_historical\n",
    "    url_transcript = r\"fomcminutes{}.htm\"\n",
    "    href_regex = re.compile(\"(?i)/fomc[/]?minutes[/]?\\d{8}.htm\")\n",
    "\n",
    "    def __init__(self, dates, nthreads=5, save_path=None):\n",
    "\n",
    "        # make sure user has given list with strings\n",
    "        if not isinstance(dates, list):\n",
    "            raise TypeError(\"dates should be a list of yyyy or yyyymm str/int\")\n",
    "\n",
    "        elif not all([bool(re.search(r\"^\\d{4}$|^\\d{6}$\", str(d))) for d in dates]):\n",
    "            raise ValueError(\"dates should be in a yyyy or yyyymm format\")\n",
    "\n",
    "        self.dates = dates\n",
    "        self.nthreads = nthreads\n",
    "        self.save_path = save_path\n",
    "\n",
    "        self.ndates = len(dates)\n",
    "        self.years = [int(d[:4]) for d in dates]\n",
    "        self.min_year, self.max_year = min(self.years), max(self.years)\n",
    "        self.transcript_dates = []\n",
    "        self.transcripts = {}\n",
    "        self.historical_date = None\n",
    "\n",
    "        self._get_transcript_dates()\n",
    "\n",
    "        self.start_threading()\n",
    "\n",
    "        if save_path:\n",
    "            self.save_transcript()\n",
    "\n",
    "    def _get_transcript_dates(self):\n",
    "        \"\"\"\n",
    "        Extract all links for\n",
    "        \"\"\"\n",
    "\n",
    "        r = requests.get(urljoin(FedMinScraper.url_parent, FedMinScraper.url_current))\n",
    "        soup = BeautifulSoup(r.text, \"lxml\")\n",
    "        # dates are given by yyyymmdd\n",
    "\n",
    "        tdates = soup.findAll(\"a\", href=self.href_regex)\n",
    "        tdates = [re.search(r\"\\d{8}\", str(t))[0] for t in tdates]\n",
    "        self.historical_date = int(min(tdates)[:4])\n",
    "        # find minimum year\n",
    "\n",
    "        # extract all of these and filter\n",
    "        # tdates can only be applied to /fomcminutes\n",
    "        # historical dates need to be applied to federalreserve.gov\n",
    "\n",
    "        if self.min_year < self.historical_date:\n",
    "            # just append the years i'm interested in\n",
    "            for y in range(self.min_year, self.historical_date + 1):\n",
    "\n",
    "                r = requests.get(\n",
    "                    urljoin(FedMinScraper.url_parent, FedMinScraper.url_historical.format(y)))\n",
    "                soup = BeautifulSoup(r.text, parser=\"lxml\")\n",
    "                hdates = soup.find_all(\"a\", href=self.href_regex)\n",
    "                tdates.extend([re.search(r\"\\d{8}\", str(t))[0] for t in hdates])\n",
    "\n",
    "        self.transcript_dates = tdates\n",
    "\n",
    "    def get_transcript(self, transcript_date):\n",
    "\n",
    "        transcript_url = urljoin(\n",
    "            FedMinScraper.url_parent,\n",
    "            FedMinScraper.url_transcript.format(transcript_date),\n",
    "        )\n",
    "        r = requests.get(transcript_url)\n",
    "\n",
    "        if not r.ok:\n",
    "            transcript_url = urljoin(\n",
    "                FedMinScraper.url_parent.replace(\"/monetarypolicy\", \"\"),\n",
    "                r\"fomc/minutes/{}.htm\".format(transcript_date),\n",
    "            )\n",
    "            r = requests.get(transcript_url)\n",
    "\n",
    "        html = r.text\n",
    "\n",
    "        # p tag is not properly closed in many cases\n",
    "        html = html.replace('<P', '<p').replace('</P>', '</p>')\n",
    "        html = html.replace('<p', '</p><p').replace('</p><p', '<p', 1)\n",
    "\n",
    "        # remove all after appendix or references\n",
    "        x = re.search(r'(<b>references|<b>appendix|<strong>references|<strong>appendix)', html.lower())\n",
    "        if x:\n",
    "            html = html[:x.start()]\n",
    "            html += '</body></html>'\n",
    "        # Parse html text by BeautifulSoup\n",
    "        article = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "        #if link == '/fomc/MINUTES/1994/19940517min.htm':\n",
    "        #print(article)\n",
    "\n",
    "        # Remove footnote\n",
    "        for fn in article.find_all('a', {'name': re.compile('fn\\d')}):\n",
    "            fn.decompose()\n",
    "        \n",
    "        #article = re.sub(r\"  \", r\" \", article)\n",
    "        #print(article)\n",
    "        # Get all p tag\n",
    "        paragraphs = article.findAll('p')\n",
    "        #print(paragraphs)\n",
    "        self.transcripts[transcript_date] = \"\\n\\n[SECTION]\\n\\n\".join([paragraph.get_text().strip() for paragraph in paragraphs])\n",
    "\n",
    "    def start_threading(self):\n",
    "        with concurrent.futures.ThreadPoolExecutor(max_workers=self.nthreads) as executor:\n",
    "            executor.map(self.get_transcript, self.transcript_dates)\n",
    "\n",
    "    def save_transcript(self):\n",
    "        if not os.path.exists(self.save_path):\n",
    "            os.makedirs(self.save_path)\n",
    "\n",
    "        for fname, txt in self.transcripts.items():\n",
    "            with open(\n",
    "                os.path.join(self.save_path, fname + \".txt\"), \"w\", encoding=\"utf-8\"\n",
    "            ) as o:\n",
    "                o.write(txt)\n",
    "                o.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "choice-thursday",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    dates = [\"1990\", \"2023\"]\n",
    "    save_path = os.path.join(os.getcwd(), \"Minutes\")\n",
    "    FMS = FedMinScraper(dates=dates, save_path=save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b61b11bd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
